---
title: "Machine Learning Model"
author: "Stat342 Team"
date: "2024-05-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(VIM)
library(dplyr)
library(skimr)
library(mice)
library(caret)
library(randomForest)
library(xgboost)
library(e1071)
library(fastR2)
library(boot)
library(datasets)
library(corrplot)
```


# Data Exploration:

```{r}
data <- read.csv("Drug_overdose_death_rates__by_drug_type__sex__age__race__and_Hispanic_origin__United_States_20240518.csv")

# See the summary of the data
skim(data)

# Visualize the pattern of missing values
aggr(data, col = c('navyblue', 'red'), numbers = TRUE, sortVars = TRUE, labels = names(data), cex.axis = 0.7, gap = 3, ylab = c("Missing data", "Pattern"))
```

We see there is no duplicate rows and 'Estimate' variable has 1111 missing values, which is 17.84% of the total data.

# Data Cleaning:

We already have some numeric variables for levels of category variables, so we decided to delete the category variables. Also, we dropped the Flag variable since it's not critical for analysis. For the missing value in Estimate, we replace them by using Multiple Imputation.

```{r,include=FALSE}
# Delete unnecessary variables
data <- data %>% select(-INDICATOR, -PANEL, -UNIT, -STUB_NAME, -STUB_LABEL, -YEAR, -AGE, -FLAG)

# Replacing Missing Value by Multiple Imputation
imputed_data <- mice(data, m = 5, method = 'pmm', seed = 500)
complete_data <- complete(imputed_data, 1)
```

# Partition data into training and test datasets:

```{r}
set.seed(123)

# Partition data into training and test sets
trainIndex <- createDataPartition(complete_data$ESTIMATE, p = 0.8, list = FALSE, times = 1)
dataTrain <- complete_data[trainIndex, ]
dataTest <- complete_data[-trainIndex, ]

# Check the dimensions of the resulting datasets
dim(dataTrain)
dim(dataTest)
```


# Train Random Forest Model:

```{r}
RF_model <- randomForest(ESTIMATE ~ ., data = dataTrain)
RF_model
```

An explained variance of 86.26% suggests that the model is performing well and is able to explain a large proportion of the variability in drug overdose death rates. 

# Evaluate Random Forest Model:

```{r}
predicted <- predict(RF_model, dataTest)

# Calculate RMSE
rmse <- sqrt(mean((dataTest$ESTIMATE - predicted)^2))

# Calculate 95% confidence interval for RMSE
n <- nrow(dataTest) # Number of observations in test data
t_value <- qt(0.975, df = n - 1) # t-value for 95% confidence interval
margin_of_error <- t_value * (rmse / sqrt(n))

# Confidence interval
lower_bound <- rmse - margin_of_error
upper_bound <- rmse + margin_of_error

print(paste("RMSE:", rmse))
print(paste("Marginal of Error:",margin_of_error))
print(paste("95% Confidence Interval for RMSE:", lower_bound, "-", upper_bound))
```

In this case, an RMSE of approximately 2.057 indicates that, on average, the model's predictions are off by about 2.057 units of the dependent variable (drug overdose death rates). Besides, with a 95% confidence level, we expect that if we were to repeat this process multiple times, approximately 95% of the time, the true RMSE would fall within the range of 1.943 to 2.172.

# Hyperparameter Tuning

To further assess the modelâ€™s performance, we tried experiments with different hyperparameters, such as the number of trees and the number of variables tried at each split, to see if the model performance can be improved.

```{r}
# Define the grid of hyperparameters to search
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5)  # Number of variables randomly sampled as candidates at each split
)

# Set up cross-validation
train_control <- trainControl(method = "cv", number = 5, search = "grid")

# Train the Random Forest model with hyperparameter tuning
set.seed(123)
tuned_rf_model <- train(
  ESTIMATE ~ PANEL_NUM + UNIT_NUM + STUB_NAME_NUM + STUB_LABEL_NUM + YEAR_NUM + AGE_NUM, 
  data = dataTrain,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  ntree = 500
)

# Print the best hyperparameters
print(paste("the best hyperparameters:",tuned_rf_model$bestTune))
```

The output indicates that the best value for the mtry parameter, as determined by the hyperparameter tuning process, is 5.

# Evaluate the Tuned Model

```{r,warning=FALSE}
# Predict on the test data using the best model
predictions <- predict(tuned_rf_model, dataTest)

# Calculate RMSE
rmse <- sqrt(mean((dataTest$ESTIMATE - predictions)^2))

# Calculate 95% confidence interval for RMSE
n <- nrow(dataTest) 
t_value <- qt(0.975, df = n - 1) 
margin_of_error <- t_value * (rmse / sqrt(n))

# Confidence interval
lower_bound <- rmse - margin_of_error
upper_bound <- rmse + margin_of_error

# Print results
cat("RMSE:", rmse, "\n")
print(paste("Marginal of Error:",margin_of_error))
cat("95% Confidence Interval for RMSE:", lower_bound, "-", upper_bound, "\n")
```

The RMSE value is 1.267378 and the margin of error is calculated as 0.07. The 95% confidence interval for RMSE is given as 1.196881 - 1.337874. This interval provides a range of values within which we can be 95% confident that the true RMSE value lies. Overall, these metrics suggest that the tuned model has a reasonably low RMSE with a narrow confidence interval, indicating good predictive performance and a relatively small margin of error. 


```{r,warning=FALSE}
# Plot the residual
residuals <- dataTest$ESTIMATE - predictions
residual_df <- data.frame(Predicted = predictions, Residuals = residuals)

ggplot(residual_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals") +
  theme_minimal()
```

It displays the difference between the observed values (actual target values) and the predicted values from the model, known as residuals, against the predicted values themselves. The residuals is randomly distributed around the horizontal line at y = 0. This indicates that the relationship between the independent variables and the dependent variable is linear. The model's performance very well.

# Exploring feature importance in a Random Forest:

```{r}
# plot feature importance
importance <- tuned_rf_model$finalModel$importance
importance_df <- as.data.frame(importance)

ggplot(importance_df, aes(x = reorder(rownames(importance_df), IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Feature Importance", x = "Feature", y = "Importance") +
  theme_minimal() +
  coord_flip()
```

A higher importance value suggests that the feature has a stronger influence on the model's predictions. In the plot, we see that Panel
has highest importance values, which are considered most informative by the model and strongly associated with higher death rates. Therefore, we can conclude that drug overdose type, age are most strongly associated with higher death rates.

