---
title: "Datathon 2024 (Health): Drug Overdose in USA "
output: 
  flexdashboard::flex_dashboard:
    theme:
      version: 4
      bootswatch: lux
    orientation: columns
    social: menu
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
library(flexdashboard)
library(readr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(shiny)
library(highcharter)
library(VIM)
library(skimr)
library(mice)
library(caret)
library(randomForest)
library(xgboost)
library(e1071)
library(fastR2)
library(boot)
library(datasets)
library(pROC)
```


Machine Learning Model
================================

Column {data-width=350}
-----------------------------------------------------------------------

```{r}
data <- read.csv("~/Desktop/Drug_overdose_death_rates__by_drug_type__sex__age__race__and_Hispanic_origin__United_States_20240518.csv")
data <- data %>% select(-INDICATOR, -PANEL, -UNIT, -STUB_NAME, -STUB_LABEL, -YEAR, -AGE, -FLAG)
  
#Multiple Imputation using mice
imputed_data <- mice(data, m = 5, method = 'pmm', seed = 500)
complete_data <- complete(imputed_data, 1)

set.seed(123)

# Partition data into training and test sets
trainIndex <- createDataPartition(complete_data$ESTIMATE, p = 0.8, list = FALSE, times = 1)
dataTrain <- complete_data[trainIndex, ]
dataTest <- complete_data[-trainIndex, ]

# Define the grid of hyperparameters to search
tune_grid <- expand.grid(
  mtry = c(2, 3, 4, 5)  # Number of variables randomly sampled as candidates at each split
)

# Set up cross-validation
train_control <- trainControl(method = "cv", number = 5, search = "grid")

# Train the Random Forest model with hyperparameter tuning
# Train the Random Forest model and suppress the unwanted output
sink("NUL")
tuned_rf_model <- train(
  ESTIMATE ~ PANEL_NUM + UNIT_NUM + STUB_NAME_NUM + STUB_LABEL_NUM + YEAR_NUM + AGE_NUM, 
  data = dataTrain,
  method = "rf",
  trControl = train_control,
  tuneGrid = tune_grid,
  ntree = 500,
  importance = TRUE
)

# Predict on the test data using the best model
predictions <- predict(tuned_rf_model, dataTest)

# Calculate RMSE
rmse <- sqrt(mean((predictions - dataTest$ESTIMATE)^2))

# Calculate R-squared
r_squared <- 1 - (sum((dataTest$ESTIMATE - predictions)^2) / sum((dataTest$ESTIMATE - mean(dataTest$ESTIMATE))^2))

# Plot residuals
residuals <- dataTest$ESTIMATE - predictions
residual_df <- data.frame(Predicted = predictions, Residuals = residuals)

```

### Model Summary

```{r}
print(tuned_rf_model)
```

### Performance Metrics

```{r}
# Print performance metrics
cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared)
```


Column {data-width=350}
-----------------------------------------------------------------------


### Feature Importance

```{r}
# plot feature importance
importance <- tuned_rf_model$finalModel$importance
importance_df <- as.data.frame(importance)

# Plot feature importance
ggplot(importance_df, aes(x = reorder(rownames(importance_df), IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Feature Importance", x = "Feature", y = "Importance") +
  theme_minimal() +
  coord_flip()
```


### Model Summary

```{r}
ggplot(residual_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residual Plot", x = "Predicted Values", y = "Residuals") +
  theme_minimal()
```

